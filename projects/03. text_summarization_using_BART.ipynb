{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/navdeep/Desktop/Generative AI/Generative-AI/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a text sample that needs to be summarized\n",
    "text = \"The foundational models are huge Neural Networks architectures that \\\n",
    "        are Pre-Trained on large amounts of unlabeled and self-supervised data, \\\n",
    "        meaning the model learns from the patterns in the data in a way that \\\n",
    "        produces generalizable and adaptable output.\\\n",
    "        And large language models are instances of foundation models applied \\\n",
    "        specifically to text and text-like things (such as essay, poetry, code, etc.) \\\n",
    "        Now LLM are trained on large datasets of text, such as books, \\\n",
    "        articles, and bunch of publicly available sources. As the name \\\n",
    "        suggests 'large' i.e. the tens of gigabytes in size (parameters) and \\\n",
    "        trained on enormous amounts if text data (certainly we’re talking \\\n",
    "        potentially petabytes of data here.)\"\n",
    "\n",
    "# load the BART model and tokenizer\n",
    "model = BartForConditionalGeneration.from_pretrained('facebook/bart-large')\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
    "\n",
    "# encode the input text using the BART tokenizer\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "\n",
    "# generate a summary of the input text using the BART model by varying max_length\n",
    "output = model.generate(input_ids, max_length=15, num_beams=4, \n",
    "                        no_repeat_ngram_size=3, early_stopping=True)\n",
    "\n",
    "# decode the output tensor and print the summarized text\n",
    "summary = tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      "The foundational models are huge Neural Networks architectures that         are Pre-Trained on large amounts of unlabeled and self-supervised data,         meaning the model learns from the patterns in the data in a way that         produces generalizable and adaptable output.        And large language models are instances of foundation models applied         specifically to text and text-like things (such as essay, poetry, code, etc.)         Now LLM are trained on large datasets of text, such as books,         articles, and bunch of publicly available sources. As the name         suggests 'large' i.e. the tens of gigabytes in size (parameters) and         trained on enormous amounts if text data (certainly we’re talking         potentially petabytes of data here.)\n",
      "============================================================\n",
      "Summarized Text:\n",
      "The foundational models are huge Neural Networks architectures that   \n"
     ]
    }
   ],
   "source": [
    "print(\"Original Text:\")\n",
    "print(text)\n",
    "print(\"==\"*30)\n",
    "print(\"Summarized Text:\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
